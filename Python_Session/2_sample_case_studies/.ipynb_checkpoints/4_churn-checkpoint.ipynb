{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Churn##\n",
    "\n",
    "Credits: Adapted from growth-workshop, as featured on the yhat blog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\"Churn Rate\" is a business term describing the rate at which customers leave or cease paying for a product or service.\n",
    "\n",
    "Predicting churn is particularly important for businesses w/ subscription models such as cell phone, cable, or merchant credit card processing plans. But modeling churn has wide reaching applications in many domains. For example, casinos have used predictive models to predict ideal room conditions for keeping patrons at the blackjack table and when to reward unlucky gamblers with front row seats to Celine Dion. Similarly, airlines may offer first class upgrades to complaining customers. The list goes on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##The Dataset##\n",
    "\n",
    "The data set we'll be using is a longstanding telecom customer data set.\n",
    "\n",
    "The data is straightforward. Each row represents a subscribing telephone customer. Each column contains customer attributes such as phone number, call minutes used during different times of day, charges incurred for services, lifetime account duration, and whether or not the customer is still a customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier as RF\n",
    "%matplotlib inline \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "churn_df = pd.read_csv('../Utils/data/churn.csv')\n",
    "col_names = churn_df.columns.tolist()\n",
    "\n",
    "print \"Column names:\"\n",
    "print col_names\n",
    "\n",
    "to_show = col_names[:6] + col_names[-6:]\n",
    "\n",
    "print \"\\nSample data:\"\n",
    "churn_df[to_show].head(6)\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be keeping the statistical model pretty simple for this example so the feature space is almost unchanged from what you see above. The following code simply drops irrelevant columns and converts strings to boolean values (since models don't handle \"yes\" and \"no\" very well). The rest of the numeric columns are left untouched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Isolate target data\n",
    "churn_result = churn_df['Churn?']\n",
    "y = np.where(churn_result == 'True.',1,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We don't need these columns\n",
    "to_drop = ['State','Area Code','Phone','Churn?']\n",
    "churn_feat_space = churn_df.drop(to_drop,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 'yes'/'no' has to be converted to boolean values\n",
    "# NumPy converts these from boolean to 1. and 0. later\n",
    "yes_no_cols = [\"Int'l Plan\",\"VMail Plan\"]\n",
    "churn_feat_space[yes_no_cols] = churn_feat_space[yes_no_cols] == 'yes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pull out features for future use\n",
    "features = churn_feat_space.columns\n",
    "print features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = churn_feat_space.as_matrix().astype(np.float)\n",
    "\n",
    "# This is important\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "print \"Feature space holds %d observations and %d features\" % X.shape\n",
    "print \"Unique target labels:\", np.unique(y)\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##How good is your model?##\n",
    "\n",
    "As a good start, cross validation will be used throught this example. Cross validation attempts to avoid overfitting (training on and predicting the same datapoint) while still producing a prediction for each observation dataset.\n",
    "\n",
    "Here's what that looks like using `scikit-learn` libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "def run_cv(X,y,clf_class,**kwargs):\n",
    "    # Construct a kfolds object\n",
    "    kf = KFold(len(y),n_folds=3,shuffle=True)\n",
    "    y_pred = y.copy()\n",
    "    \n",
    "    # Iterate through folds\n",
    "    for train_index, test_index in kf:\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train = y[train_index]\n",
    "        # Initialize a classifier with key word arguments\n",
    "        clf = clf_class(**kwargs)\n",
    "        clf.fit(X_train,y_train)\n",
    "        y_pred[test_index] = clf.predict(X_test)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare three fairly unique algorithms support vector machines, random forest, and k-nearest-neighbors. Nothing fancy here, just passing each to cross validation and determining how often the classifier predicted the correct class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier as RF\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.ensemble import GradientBoostingClassifier as GBC\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "def accuracy(y_true,y_pred):\n",
    "    # NumPy interpretes True and False as 1. and 0.\n",
    "    return np.mean(y_true == y_pred)\n",
    "\n",
    "print \"Logistic Regression:\"\n",
    "print \"%.3f\" % accuracy(y, run_cv(X,y,LR))\n",
    "print \"Gradient Boosting Classifier\"\n",
    "print \"%.3f\" % accuracy(y, run_cv(X,y,GBC))\n",
    "print \"Support vector machines:\"\n",
    "print \"%.3f\" % accuracy(y, run_cv(X,y,SVC))\n",
    "print \"Random forest:\"\n",
    "print \"%.3f\" % accuracy(y, run_cv(X,y,RF))\n",
    "print \"K-nearest-neighbors:\"\n",
    "print \"%.3f\" % accuracy(y, run_cv(X,y,KNN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Gradient Boosting performs the best!\n",
    "\n",
    "##Precision and recall##\n",
    "\n",
    "The problem with accuracy is that outcomes aren't necessarily equal. \n",
    "\n",
    "**Precision**:If my classifier predicted a customer would churn and they didn't, that's not the best but it's forgivable. \n",
    "\n",
    "**Recall**:If my classifier predicted a customer would return, I didn't act, and then they churned, that's really bad.\n",
    "\n",
    "We'll be using another built in `scikit-learn` function to construction a **confusion matrix**. \n",
    "\n",
    "A confusion matrix is a way of visualizing predictions made by a classifier and is just a table showing the distribution of predictions for a specific class. The x-axis indicates the true class of each observation (if a customer churned or not) while the y-axis corresponds to the class predicted by the model (if my classifier said a customer would churned or not).\n",
    "\n",
    "## Confusion matrix and confusion tables: \n",
    "The columns represent the actual class and the rows represent the predicted class. Lets evaluate performance: \n",
    "\n",
    "|      | condition True | condition false|\n",
    "|------|----------------|---------------|\n",
    "|prediction true|True Positive|False positive|\n",
    "|Prediction False|False Negative|True Negative|\n",
    "\n",
    "Sensitivity, Recall or True Positive Rate quantify the models ability to predict our positive classes. \n",
    "\n",
    "$$TPR = \\frac{ TP}{TP + FN}$$ \n",
    "\n",
    "Specificity or True Negative Rate quantify the models ability to predict our Negative classes. \n",
    "\n",
    "$$TNR = \\frac{ TN}{FP + TN}$$ \n",
    "\n",
    "### Example:\n",
    "\n",
    "|      | Spam | Ham|\n",
    "|------|----------------|---------------|\n",
    "|prediction Spam|100|50|\n",
    "|Prediction Ham|75|900|\n",
    "\n",
    "$$TPR = \\frac{100}{100 + 75} = 57.14 \\%   Sensitive $$\n",
    "\n",
    "$$TNR = \\frac{ 900}{50 + 900} = 94.73 \\% Specific $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "def draw_confusion_matrices(confusion_matricies,class_names):\n",
    "    class_names = class_names.tolist()\n",
    "    for cm in confusion_matrices:\n",
    "        classifier, cm = cm[0], cm[1]\n",
    "        print(cm)\n",
    "        \n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "        cax = ax.matshow(cm)\n",
    "        plt.title('Confusion matrix for %s' % classifier)\n",
    "        fig.colorbar(cax)\n",
    "        ax.set_xticklabels([''] + class_names)\n",
    "        ax.set_yticklabels([''] + class_names)\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.show()\n",
    "    \n",
    "y = np.array(y)\n",
    "class_names = np.unique(y)\n",
    "\n",
    "confusion_matrices = [\n",
    "    ( \"Support Vector Machines\", confusion_matrix(y,run_cv(X,y,SVC)) ),\n",
    "    ( \"Random Forest\", confusion_matrix(y,run_cv(X,y,RF)) ),\n",
    "    ( \"K-Nearest-Neighbors\", confusion_matrix(y,run_cv(X,y,KNN)) ),\n",
    "    ( \"Gradient Boosting Classifier\", confusion_matrix(y,run_cv(X,y,GBC)) ),\n",
    "    ( \"Logisitic Regression\", confusion_matrix(y,run_cv(X,y,LR)) )\n",
    "]\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "draw_confusion_matrices(confusion_matrices,class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "\n",
    "While, just like accuracy, precision and recall still rank random forest above SVC and KNN, this won't always be true. When different measurements do return a different pecking order, understanding the values and tradeoffs of each rating should effect how you proceed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Plots & AUC\n",
    "\n",
    "Another important metric to consider is ROC plots. \n",
    "\n",
    "To explain with a little more detail, a ROC curve plots the true positives (sensitivity) vs. false positives (1 âˆ’ specificity), for a binary classifier system as its discrimination threshold is varied. They have better expected performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy import interp\n",
    "\n",
    "def plot_roc(X, y, clf_class, **kwargs):\n",
    "    kf = KFold(len(y), n_folds=5, shuffle=True)\n",
    "    y_prob = np.zeros((len(y),2))\n",
    "    mean_tpr = 0.0\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    all_tpr = []\n",
    "    for i, (train_index, test_index) in enumerate(kf):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train = y[train_index]\n",
    "        clf = clf_class(**kwargs)\n",
    "        clf.fit(X_train,y_train)\n",
    "        # Predict probabilities, not classes\n",
    "        y_prob[test_index] = clf.predict_proba(X_test)\n",
    "        fpr, tpr, thresholds = roc_curve(y[test_index], y_prob[test_index, 1])\n",
    "        mean_tpr += interp(mean_fpr, fpr, tpr)\n",
    "        mean_tpr[0] = 0.0\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, lw=1, label='ROC fold %d (area = %0.2f)' % (i, roc_auc))\n",
    "    mean_tpr /= len(kf)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "    plt.plot(mean_fpr, mean_tpr, 'k--',label='Mean ROC (area = %0.2f)' % mean_auc, lw=2)\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], '--', color=(0.6, 0.6, 0.6), label='Random')\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "      \n",
    "\n",
    "print \"Support vector machines:\"\n",
    "plot_roc(X,y,SVC,probability=True)\n",
    "\n",
    "print \"Random forests:\"\n",
    "plot_roc(X,y,RF,n_estimators=18)\n",
    "\n",
    "print \"K-nearest-neighbors:\"\n",
    "plot_roc(X,y,KNN)\n",
    "\n",
    "print \"Gradient Boosting Classifier:\"\n",
    "plot_roc(X,y,GBC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance\n",
    "\n",
    "Now that we understand the accuracy of each individual model for our particular dataset, let's dive a little deeper to get a better understanding of what features or behaviours are causing our customers to churn. \n",
    "\n",
    "In the next section, we will be using a `RandomForestClassifer` to build an ensemble of decision trees to predict whether a customer will churn or not churn. \n",
    "\n",
    "Let's look at the Top 10 features in our dataset that contribute to customer churn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_index,test_index = train_test_split(churn_df.index)\n",
    "\n",
    "forest = RF()\n",
    "forest_fit = forest.fit(X[train_index], y[train_index])\n",
    "forest_predictions = forest_fit.predict(X[test_index])\n",
    "\n",
    "importances = forest_fit.feature_importances_[:10]\n",
    "std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(10):\n",
    "    print(\"%d. %s (%f)\" % (f + 1, features[f], importances[indices[f]]))\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "#import pylab as pl\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(10), importances[indices], yerr=std[indices], color=\"r\", align=\"center\")\n",
    "plt.xticks(range(10), indices)\n",
    "plt.xlim([-1, 10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
