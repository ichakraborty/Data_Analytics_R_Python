{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A deep neural network with or w/o dropout in one file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import theano\n",
    "import sys\n",
    "from theano import tensor as T\n",
    "from theano import shared\n",
    "from theano.tensor.shared_randomstreams import RandomStreams\n",
    "from collections import OrderedDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu_f(vec):\n",
    "    \"\"\" Wrapper to quickly change the rectified linear unit function \"\"\"\n",
    "    return (vec + abs(vec)) / 2.\n",
    "\n",
    "\n",
    "def dropout(rng, x, p=0.5):\n",
    "    \"\"\" Zero-out random values in x with probability p using rng \"\"\"\n",
    "    if p > 0. and p < 1.:\n",
    "        seed = rng.randint(2 ** 30)\n",
    "        srng = theano.tensor.shared_randomstreams.RandomStreams(seed)\n",
    "        mask = srng.binomial(n=1, p=1.-p, size=x.shape,\n",
    "                dtype=theano.config.floatX)\n",
    "        return x * mask\n",
    "    return x\n",
    "\n",
    "\n",
    "def build_shared_zeros(shape, name):\n",
    "    \"\"\" Builds a theano shared variable filled with a zeros numpy array \"\"\"\n",
    "    return shared(value=numpy.zeros(shape, dtype=theano.config.floatX),\n",
    "            name=name, borrow=True)\n",
    "\n",
    "\n",
    "class Linear(object):\n",
    "    \"\"\" Basic linear transformation layer (W.X + b) \"\"\"\n",
    "    def __init__(self, rng, input, n_in, n_out, W=None, b=None):\n",
    "        if W is None:\n",
    "            W_values = numpy.asarray(rng.uniform(\n",
    "                low=-numpy.sqrt(6. / (n_in + n_out)),\n",
    "                high=numpy.sqrt(6. / (n_in + n_out)),\n",
    "                size=(n_in, n_out)), dtype=theano.config.floatX)\n",
    "            W_values *= 4  # This works for sigmoid activated networks!\n",
    "            W = theano.shared(value=W_values, name='W', borrow=True)\n",
    "        if b is None:\n",
    "            b = build_shared_zeros((n_out,), 'b')\n",
    "        self.input = input\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.params = [self.W, self.b]\n",
    "        self.output = T.dot(self.input, self.W) + self.b\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Linear\"\n",
    "\n",
    "\n",
    "class SigmoidLayer(Linear):\n",
    "    \"\"\" Sigmoid activation layer (sigmoid(W.X + b)) \"\"\"\n",
    "    def __init__(self, rng, input, n_in, n_out, W=None, b=None):\n",
    "        super(SigmoidLayer, self).__init__(rng, input, n_in, n_out, W, b)\n",
    "        self.output = T.nnet.sigmoid(self.output)\n",
    "\n",
    "\n",
    "class ReLU(Linear):\n",
    "    \"\"\" Rectified Linear Unit activation layer (max(0, W.X + b)) \"\"\"\n",
    "    def __init__(self, rng, input, n_in, n_out, W=None, b=None):\n",
    "        if b is None:\n",
    "            b = build_shared_zeros((n_out,), 'b')\n",
    "        super(ReLU, self).__init__(rng, input, n_in, n_out, W, b)\n",
    "        self.output = relu_f(self.output)\n",
    "\n",
    "\n",
    "class DatasetMiniBatchIterator(object):\n",
    "    \"\"\" Basic mini-batch iterator \"\"\"\n",
    "    def __init__(self, x, y, batch_size=100):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i in xrange((self.x.shape[0]+self.batch_size-1)\n",
    "                        / self.batch_size):\n",
    "            yield (self.x[i*self.batch_size:(i+1)*self.batch_size],\n",
    "                   self.y[i*self.batch_size:(i+1)*self.batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    \"\"\"Multi-class Logistic Regression\n",
    "    \"\"\"\n",
    "    def __init__(self, rng, input, n_in, n_out, W=None, b=None):\n",
    "        if W != None:\n",
    "            self.W = W\n",
    "        else:\n",
    "            self.W = build_shared_zeros((n_in, n_out), 'W')\n",
    "        if b != None:\n",
    "            self.b = b\n",
    "        else:\n",
    "            self.b = build_shared_zeros((n_out,), 'b')\n",
    "\n",
    "        # P(Y|X) = softmax(W.X + b)\n",
    "        self.p_y_given_x = T.nnet.softmax(T.dot(input, self.W) + self.b)\n",
    "        self.y_pred = T.argmax(self.p_y_given_x, axis=1)\n",
    "        self.output = self.y_pred\n",
    "        self.params = [self.W, self.b]\n",
    "\n",
    "    def negative_log_likelihood(self, y):\n",
    "        return -T.mean(T.log(self.p_y_given_x)[T.arange(y.shape[0]), y])\n",
    "\n",
    "    def negative_log_likelihood_sum(self, y):\n",
    "        return -T.sum(T.log(self.p_y_given_x)[T.arange(y.shape[0]), y])\n",
    "\n",
    "    def training_cost(self, y):\n",
    "        \"\"\" Wrapper for standard name \"\"\"\n",
    "        return self.negative_log_likelihood_sum(y)\n",
    "\n",
    "    def errors(self, y):\n",
    "        if y.ndim != self.y_pred.ndim:\n",
    "            raise TypeError(\"y should have the same shape as self.y_pred\",\n",
    "                (\"y\", y.type, \"y_pred\", self.y_pred.type))\n",
    "        if y.dtype.startswith('int'):\n",
    "            return T.mean(T.neq(self.y_pred, y))\n",
    "        else:\n",
    "            print(\"!!! y should be of int type\")\n",
    "            return T.mean(T.neq(self.y_pred, numpy.asarray(y, dtype='int')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NeuralNet(object):\n",
    "    \"\"\" Neural network (not regularized, without dropout) \"\"\"\n",
    "    def __init__(self, numpy_rng, theano_rng=None, \n",
    "                 n_ins=40*3,\n",
    "                 layers_types=[Linear, ReLU, ReLU, ReLU, LogisticRegression],\n",
    "                 layers_sizes=[1024, 1024, 1024, 1024],\n",
    "                 n_outs=62 * 3,\n",
    "                 rho=0.9, eps=1.E-6,\n",
    "                 debugprint=False):\n",
    "        \"\"\"\n",
    "        TODO\n",
    "        \"\"\"\n",
    "        self.layers = []\n",
    "        self.params = []\n",
    "        self.n_layers = len(layers_types)\n",
    "        self.layers_types = layers_types\n",
    "        assert self.n_layers > 0\n",
    "        self._rho = rho  # ``momentum'' for adadelta\n",
    "        self._eps = eps  # epsilon for adadelta\n",
    "        self._accugrads = []  # for adadelta\n",
    "        self._accudeltas = []  # for adadelta\n",
    "\n",
    "        if theano_rng == None:\n",
    "            theano_rng = RandomStreams(numpy_rng.randint(2 ** 30))\n",
    "\n",
    "        self.x = T.fmatrix('x')\n",
    "        self.y = T.ivector('y')\n",
    "        \n",
    "        self.layers_ins = [n_ins] + layers_sizes\n",
    "        self.layers_outs = layers_sizes + [n_outs]\n",
    "        \n",
    "        layer_input = self.x\n",
    "        \n",
    "        for layer_type, n_in, n_out in zip(layers_types,\n",
    "                self.layers_ins, self.layers_outs):\n",
    "            this_layer = layer_type(rng=numpy_rng,\n",
    "                    input=layer_input, n_in=n_in, n_out=n_out)\n",
    "            assert hasattr(this_layer, 'output')\n",
    "            self.params.extend(this_layer.params)\n",
    "            self._accugrads.extend([build_shared_zeros(t.shape.eval(),\n",
    "                'accugrad') for t in this_layer.params])\n",
    "            self._accudeltas.extend([build_shared_zeros(t.shape.eval(),\n",
    "                'accudelta') for t in this_layer.params])\n",
    "            self.layers.append(this_layer)\n",
    "            layer_input = this_layer.output\n",
    "\n",
    "        assert hasattr(self.layers[-1], 'training_cost')\n",
    "        assert hasattr(self.layers[-1], 'errors')\n",
    "        # TODO standardize cost\n",
    "        self.mean_cost = self.layers[-1].negative_log_likelihood(self.y)\n",
    "        self.cost = self.layers[-1].training_cost(self.y)\n",
    "        if debugprint:\n",
    "            theano.printing.debugprint(self.cost)\n",
    "\n",
    "        self.errors = self.layers[-1].errors(self.y)\n",
    "\n",
    "    def __repr__(self):\n",
    "        dimensions_layers_str = map(lambda x: \"x\".join(map(str, x)),\n",
    "                                    zip(self.layers_ins, self.layers_outs))\n",
    "        return \"_\".join(map(lambda x: \"_\".join((x[0].__name__, x[1])),\n",
    "                            zip(self.layers_types, dimensions_layers_str)))\n",
    "\n",
    "\n",
    "    def get_SGD_trainer(self):\n",
    "        \"\"\" Returns a plain SGD minibatch trainer with learning rate as param.\n",
    "        \"\"\"\n",
    "        batch_x = T.fmatrix('batch_x')\n",
    "        batch_y = T.ivector('batch_y')\n",
    "        learning_rate = T.fscalar('lr')  # learning rate to use\n",
    "        # compute the gradients with respect to the model parameters\n",
    "        # using mean_cost so that the learning rate is not too dependent\n",
    "        # on the batch size\n",
    "        gparams = T.grad(self.mean_cost, self.params)\n",
    "\n",
    "        # compute list of weights updates\n",
    "        updates = OrderedDict()\n",
    "        for param, gparam in zip(self.params, gparams):\n",
    "            updates[param] = param - gparam * learning_rate\n",
    "\n",
    "        train_fn = theano.function(inputs=[theano.Param(batch_x),\n",
    "                                           theano.Param(batch_y),\n",
    "                                           theano.Param(learning_rate)],\n",
    "                                   outputs=self.mean_cost,\n",
    "                                   updates=updates,\n",
    "                                   givens={self.x: batch_x, self.y: batch_y})\n",
    "\n",
    "        return train_fn\n",
    "\n",
    "    def get_adadelta_trainer(self):\n",
    "        \"\"\" Returns an Adadelta (Zeiler 2012) trainer using self._rho and\n",
    "        self._eps params.\n",
    "        \"\"\"\n",
    "        batch_x = T.fmatrix('batch_x')\n",
    "        batch_y = T.ivector('batch_y')\n",
    "        # compute the gradients with respect to the model parameters\n",
    "        gparams = T.grad(self.cost, self.params)\n",
    "\n",
    "        # compute list of weights updates\n",
    "        updates = OrderedDict()\n",
    "        for accugrad, accudelta, param, gparam in zip(self._accugrads,\n",
    "                self._accudeltas, self.params, gparams):\n",
    "            # c.f. Algorithm 1 in the Adadelta paper (Zeiler 2012)\n",
    "            agrad = self._rho * accugrad + (1 - self._rho) * gparam * gparam\n",
    "            dx = - T.sqrt((accudelta + self._eps)\n",
    "                          / (agrad + self._eps)) * gparam\n",
    "            updates[accudelta] = (self._rho * accudelta\n",
    "                                  + (1 - self._rho) * dx * dx)\n",
    "            updates[param] = param + dx\n",
    "            updates[accugrad] = agrad\n",
    "\n",
    "        train_fn = theano.function(inputs=[theano.Param(batch_x),\n",
    "                                           theano.Param(batch_y)],\n",
    "                                   outputs=self.cost,\n",
    "                                   updates=updates,\n",
    "                                   givens={self.x: batch_x, self.y: batch_y})\n",
    "\n",
    "        return train_fn\n",
    "\n",
    "    def score_classif(self, given_set):\n",
    "        \"\"\" Returns functions to get current classification errors. \"\"\"\n",
    "        batch_x = T.fmatrix('batch_x')\n",
    "        batch_y = T.ivector('batch_y')\n",
    "        score = theano.function(inputs=[theano.Param(batch_x),\n",
    "                                        theano.Param(batch_y)],\n",
    "                                outputs=self.errors,\n",
    "                                givens={self.x: batch_x, self.y: batch_y})\n",
    "\n",
    "        def scoref():\n",
    "            \"\"\" returned function that scans the entire set given as input \"\"\"\n",
    "            return [score(batch_x, batch_y) for batch_x, batch_y in given_set]\n",
    "\n",
    "        return scoref\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RegularizedNet(NeuralNet):\n",
    "    \"\"\" Neural net with L1 and L2 regularization \"\"\"\n",
    "    def __init__(self, numpy_rng, theano_rng=None,\n",
    "                 n_ins=100,\n",
    "                 layers_types=[ReLU, ReLU, ReLU, LogisticRegression],\n",
    "                 layers_sizes=[1024, 1024, 1024],\n",
    "                 n_outs=2,\n",
    "                 rho=0.9, eps=1.E-6,\n",
    "                 L1_reg=0.,\n",
    "                 L2_reg=0.,\n",
    "                 debugprint=False):\n",
    "        \"\"\"\n",
    "        TODO\n",
    "        \"\"\"\n",
    "        super(RegularizedNet, self).__init__(numpy_rng, theano_rng, n_ins,\n",
    "                layers_types, layers_sizes, n_outs, rho, eps, debugprint)\n",
    "\n",
    "        L1 = shared(0.)\n",
    "        for param in self.params:\n",
    "            L1 += T.sum(abs(param))\n",
    "        if L1_reg > 0.:\n",
    "            self.cost = self.cost + L1_reg * L1\n",
    "        L2 = shared(0.)\n",
    "        for param in self.params:\n",
    "            L2 += T.sum(param ** 2)\n",
    "        if L2_reg > 0.:\n",
    "            self.cost = self.cost + L2_reg * L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DropoutNet(NeuralNet):\n",
    "    \"\"\" Neural net with dropout (see Hinton's et al. paper) \"\"\"\n",
    "    def __init__(self, numpy_rng, theano_rng=None,\n",
    "                 n_ins=40*3,\n",
    "                 layers_types=[Linear, ReLU, ReLU, ReLU, LogisticRegression],\n",
    "                 layers_sizes=[1024, 1024, 1024, 1024],\n",
    "                 dropout_rates=[0.2, 0.5, 0.5, 0.5, 0.5],\n",
    "                 n_outs=62 * 3,\n",
    "                 rho=0.9, eps=1.E-6,\n",
    "                 debugprint=False):\n",
    "        \"\"\"\n",
    "        TODO\n",
    "        \"\"\"\n",
    "        super(DropoutNet, self).__init__(numpy_rng, theano_rng, n_ins,\n",
    "                layers_types, layers_sizes, n_outs, rho, eps, debugprint)\n",
    "\n",
    "        self.dropout_rates = dropout_rates\n",
    "        dropout_layer_input = dropout(numpy_rng, self.x, p=dropout_rates[0])\n",
    "        self.dropout_layers = []\n",
    "\n",
    "        for layer, layer_type, n_in, n_out, dr in zip(self.layers,\n",
    "                layers_types, self.layers_ins, self.layers_outs,\n",
    "                dropout_rates[1:] + [0]):  # !!! we do not dropout anything\n",
    "                                            # from the last layer !!!\n",
    "            this_layer = layer_type(rng=numpy_rng,\n",
    "                    input=dropout_layer_input, n_in=n_in, n_out=n_out,\n",
    "                    W=layer.W * 1. / (1. - dr), # experimental\n",
    "                    b=layer.b * 1. / (1. - dr)) # TODO check\n",
    "            assert hasattr(this_layer, 'output')\n",
    "            # N.B. dropout with dr==1 does not dropanything!!\n",
    "            this_layer.output = dropout(numpy_rng, this_layer.output, dr)\n",
    "            self.dropout_layers.append(this_layer)\n",
    "            dropout_layer_input = this_layer.output\n",
    "\n",
    "        assert hasattr(self.layers[-1], 'training_cost')\n",
    "        assert hasattr(self.layers[-1], 'errors')\n",
    "        # TODO standardize cost\n",
    "        # these are the dropout costs\n",
    "        self.mean_cost = self.dropout_layers[-1].negative_log_likelihood(self.y)\n",
    "        self.cost = self.dropout_layers[-1].training_cost(self.y)\n",
    "\n",
    "        # these is the non-dropout errors\n",
    "        self.errors = self.layers[-1].errors(self.y)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return super(DropoutNet, self).__repr__() + \"\\n\"\\\n",
    "                + \"dropout rates: \" + str(self.dropout_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_fit_and_score(class_to_chg):\n",
    "    \"\"\" Mutates a class to add the fit() and score() functions to a NeuralNet.\n",
    "    \"\"\"\n",
    "    from types import MethodType\n",
    "    def fit(self, x_train, y_train, x_dev=None, y_dev=None,\n",
    "            max_epochs=100, early_stopping=True, split_ratio=0.1,\n",
    "            verbose=False):\n",
    "        \"\"\"\n",
    "        TODO\n",
    "        \"\"\"\n",
    "        import time, copy\n",
    "        if x_dev == None or y_dev == None:\n",
    "            from sklearn.cross_validation import train_test_split\n",
    "            x_train, x_dev, y_train, y_dev = train_test_split(x_train, y_train,\n",
    "                    test_size=split_ratio, random_state=42)\n",
    "        train_fn = self.get_adadelta_trainer()\n",
    "        train_set_iterator = DatasetMiniBatchIterator(x_train, y_train)\n",
    "        dev_set_iterator = DatasetMiniBatchIterator(x_dev, y_dev)\n",
    "        train_scoref = self.score_classif(train_set_iterator)\n",
    "        dev_scoref = self.score_classif(dev_set_iterator)\n",
    "        best_dev_loss = numpy.inf\n",
    "        epoch = 0\n",
    "        # TODO early stopping (not just cross val, also stop training)\n",
    "        while epoch < max_epochs:\n",
    "            if not verbose:\n",
    "                sys.stdout.write(\"\\r%0.2f%%\" % (epoch * 100./ max_epochs))\n",
    "                sys.stdout.flush()\n",
    "            avg_costs = []\n",
    "            timer = time.time()\n",
    "            for x, y in train_set_iterator:\n",
    "                avg_cost = train_fn(x, y)\n",
    "                if type(avg_cost) == list:\n",
    "                    avg_costs.append(avg_cost[0])\n",
    "                else:\n",
    "                    avg_costs.append(avg_cost)\n",
    "            if verbose:\n",
    "                print('  epoch %i took %f seconds' %\n",
    "                      (epoch, time.time() - timer))\n",
    "                print('  epoch %i, avg costs %f' %\n",
    "                      (epoch, numpy.mean(avg_costs)))\n",
    "                print('  epoch %i, training error %f' %\n",
    "                      (epoch, numpy.mean(train_scoref())))\n",
    "            dev_errors = numpy.mean(dev_scoref())\n",
    "            if dev_errors < best_dev_loss:\n",
    "                best_dev_loss = dev_errors\n",
    "                best_params = copy.deepcopy(self.params)\n",
    "                if verbose:\n",
    "                    print('!!!  epoch %i, validation error of best model %f' %\n",
    "                          (epoch, dev_errors))\n",
    "            epoch += 1\n",
    "        if not verbose:\n",
    "            print(\"\")\n",
    "        for i, param in enumerate(best_params):\n",
    "            self.params[i] = param\n",
    "\n",
    "    def score(self, x, y):\n",
    "        \"\"\" error rates \"\"\"\n",
    "        iterator = DatasetMiniBatchIterator(x, y)\n",
    "        scoref = self.score_classif(iterator)\n",
    "        return numpy.mean(scoref())\n",
    "\n",
    "    class_to_chg.fit = MethodType(fit, None, class_to_chg)\n",
    "    class_to_chg.score = MethodType(score, None, class_to_chg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dataset size:\n",
      "n samples: 1288\n",
      "n features: 1850\n",
      "n classes: 7\n",
      "Linear SVM\n",
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.001, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "score: 0.726708"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/atrivedi/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.py:167: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n",
      "/Users/atrivedi/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.py:184: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n",
      "  warnings.warn(\"Numerical issues were encountered \"\n",
      "/Users/atrivedi/anaconda/lib/python2.7/site-packages/sklearn/utils/class_weight.py:62: DeprecationWarning: The class_weight='auto' heuristic is deprecated in 0.17 in favor of a new heuristic class_weight='balanced'. 'auto' will be removed in 0.19\n",
      "  \" 0.19\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RBF-kernel SVM\n",
      "SVC(C=1.0, cache_size=200, class_weight='auto', coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "score: 0.748447\n",
      "Dropout DNN\n",
      "ReLU_1850x1000_ReLU_1000x1000_ReLU_1000x1000_LogisticRegression_1000x7\n",
      "dropout rates: [0.0, 0.5, 0.5, 0.5]\n",
      "6.25%"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-cea1107e99a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         train_models(x_train, y_train, x_test, y_test, X.shape[1],\n\u001b[0;32m--> 126\u001b[0;31m                      len(set(y)), numpy_rng=numpy.random.RandomState(123))\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mTWENTYNEWSGROUPS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-cea1107e99a2>\u001b[0m in \u001b[0;36mtrain_models\u001b[0;34m(x_train, y_train, x_test, y_test, n_features, n_outs, use_dropout, n_epochs, numpy_rng, svms, nb, deepnn)\u001b[0m\n\u001b[1;32m     89\u001b[0m                         debugprint=0)\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdnn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mdnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mVERBOSE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"score: %f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-bf2929a3d931>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x_train, y_train, x_dev, y_dev, max_epochs, early_stopping, split_ratio, verbose)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mtimer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_set_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                 \u001b[0mavg_cost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_cost\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                     \u001b[0mavg_costs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_cost\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/atrivedi/anaconda/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    593\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    596\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'position_of_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    add_fit_and_score(DropoutNet)\n",
    "    add_fit_and_score(RegularizedNet)\n",
    "\n",
    "    def nudge_dataset(X, Y):\n",
    "        \"\"\"\n",
    "        This produces a dataset 5 times bigger than the original one,\n",
    "        by moving the 8x8 images in X around by 1px to left, right, down, up\n",
    "        \"\"\"\n",
    "        from scipy.ndimage import convolve\n",
    "        direction_vectors = [\n",
    "            [[0, 1, 0],\n",
    "             [0, 0, 0],\n",
    "             [0, 0, 0]],\n",
    "            [[0, 0, 0],\n",
    "             [1, 0, 0],\n",
    "             [0, 0, 0]],\n",
    "            [[0, 0, 0],\n",
    "             [0, 0, 1],\n",
    "             [0, 0, 0]],\n",
    "            [[0, 0, 0],\n",
    "             [0, 0, 0],\n",
    "             [0, 1, 0]]]\n",
    "        shift = lambda x, w: convolve(x.reshape((8, 8)), mode='constant',\n",
    "                                      weights=w).ravel()\n",
    "        X = numpy.concatenate([X] +\n",
    "                              [numpy.apply_along_axis(shift, 1, X, vector)\n",
    "                                  for vector in direction_vectors])\n",
    "        Y = numpy.concatenate([Y for _ in range(5)], axis=0)\n",
    "        return X, Y\n",
    "\n",
    "    from sklearn import datasets, svm, naive_bayes\n",
    "    from sklearn import cross_validation, preprocessing\n",
    "    DIGITS = False\n",
    "    FACES = True\n",
    "    TWENTYNEWSGROUPS = True\n",
    "    VERBOSE = False\n",
    "    SCALE = True\n",
    "\n",
    "    def train_models(x_train, y_train, x_test, y_test, n_features, n_outs,\n",
    "            use_dropout=True, n_epochs=100, numpy_rng=None,\n",
    "            svms=True, nb=False, deepnn=True):\n",
    "        if svms:\n",
    "            print(\"Linear SVM\")\n",
    "            classifier = svm.SVC(gamma=0.001)\n",
    "            print(classifier)\n",
    "            classifier.fit(x_train, y_train)\n",
    "            print(\"score: %f\" % classifier.score(x_test, y_test))\n",
    "\n",
    "            print(\"RBF-kernel SVM\")\n",
    "            classifier = svm.SVC(kernel='rbf', class_weight='balanced')\n",
    "            print(classifier)\n",
    "            classifier.fit(x_train, y_train)\n",
    "            print(\"score: %f\" % classifier.score(x_test, y_test))\n",
    "\n",
    "        if nb:\n",
    "            print(\"Multinomial Naive Bayes\")\n",
    "            classifier = naive_bayes.MultinomialNB()\n",
    "            print(classifier)\n",
    "            classifier.fit(x_train, y_train)\n",
    "            print(\"score: %f\" % classifier.score(x_test, y_test))\n",
    "\n",
    "        if deepnn:\n",
    "            if use_dropout:\n",
    "                print(\"Dropout DNN\")\n",
    "                dnn = DropoutNet(numpy_rng=numpy_rng, n_ins=n_features,\n",
    "                        #layers_types=[LogisticRegression],\n",
    "                        #layers_sizes=[],\n",
    "                        #dropout_rates=[0.],\n",
    "                        layers_types=[ReLU, ReLU, ReLU, LogisticRegression],\n",
    "                        layers_sizes=[1000, 1000, 1000],\n",
    "                        dropout_rates=[0., 0.5, 0.5, 0.5],\n",
    "                        #layers_types=[ReLU, ReLU, LogisticRegression],\n",
    "                        #layers_sizes=[200, 200],\n",
    "                        #dropout_rates=[0., 0.5, 0.5],\n",
    "                        n_outs=n_outs,\n",
    "                        debugprint=0)\n",
    "                n_epochs *= 4\n",
    "            else:\n",
    "                print(\"Simple (regularized) DNN\")\n",
    "                dnn = RegularizedNet(numpy_rng=numpy_rng, n_ins=n_features,\n",
    "                        layers_types=[ReLU, ReLU, ReLU, LogisticRegression],\n",
    "                        layers_sizes=[1000, 1000, 1000],\n",
    "                        #layers_types=[ReLU, LogisticRegression],\n",
    "                        #layers_sizes=[1000],\n",
    "                        n_outs=n_outs,\n",
    "                        L1_reg=0.001/x_train.shape[0],\n",
    "                        L2_reg=0.001/x_train.shape[0],\n",
    "                        debugprint=0)\n",
    "            print(dnn)\n",
    "            dnn.fit(x_train, y_train, max_epochs=n_epochs, verbose=VERBOSE)\n",
    "            print(\"score: %f\" % (1. - dnn.score(x_test, y_test)))\n",
    "\n",
    "\n",
    "    if DIGITS:\n",
    "        digits = datasets.load_digits()\n",
    "        data = numpy.asarray(digits.data, dtype='float32')\n",
    "        target = numpy.asarray(digits.target, dtype='int32')\n",
    "        nudged_x, nudged_y = nudge_dataset(data, target)\n",
    "        if SCALE:\n",
    "            nudged_x = preprocessing.scale(nudged_x)\n",
    "        x_train, x_test, y_train, y_test = cross_validation.train_test_split(\n",
    "                nudged_x, nudged_y, test_size=0.3, random_state=42)\n",
    "        train_models(x_train, y_train, x_test, y_test, nudged_x.shape[1],\n",
    "                     len(set(target)), numpy_rng=numpy.random.RandomState(123))\n",
    "\n",
    "    if FACES:\n",
    "        import logging\n",
    "        logging.basicConfig(level=logging.INFO,\n",
    "                            format='%(asctime)s %(message)s')\n",
    "        lfw_people = datasets.fetch_lfw_people(min_faces_per_person=70,\n",
    "                                               resize=0.4)\n",
    "        X = numpy.asarray(lfw_people.data, dtype='float32')\n",
    "        if SCALE:\n",
    "            X = preprocessing.scale(X)\n",
    "        y = numpy.asarray(lfw_people.target, dtype='int32')\n",
    "        target_names = lfw_people.target_names\n",
    "        print(\"Total dataset size:\")\n",
    "        print(\"n samples: %d\" % X.shape[0])\n",
    "        print(\"n features: %d\" % X.shape[1])\n",
    "        print(\"n classes: %d\" % target_names.shape[0])\n",
    "        x_train, x_test, y_train, y_test = cross_validation.train_test_split(\n",
    "                    X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "        train_models(x_train, y_train, x_test, y_test, X.shape[1],\n",
    "                     len(set(y)), numpy_rng=numpy.random.RandomState(123))\n",
    "\n",
    "    if TWENTYNEWSGROUPS:\n",
    "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "        newsgroups_train = datasets.fetch_20newsgroups(subset='train')\n",
    "        vectorizer = TfidfVectorizer(encoding='latin-1', max_features=10000)\n",
    "        #vectorizer = HashingVectorizer(encoding='latin-1')\n",
    "        x_train = vectorizer.fit_transform(newsgroups_train.data)\n",
    "        x_train = numpy.asarray(x_train.todense(), dtype='float32')\n",
    "        y_train = numpy.asarray(newsgroups_train.target, dtype='int32')\n",
    "        newsgroups_test = datasets.fetch_20newsgroups(subset='test')\n",
    "        x_test = vectorizer.transform(newsgroups_test.data)\n",
    "        x_test = numpy.asarray(x_test.todense(), dtype='float32')\n",
    "        y_test = numpy.asarray(newsgroups_test.target, dtype='int32')\n",
    "        train_models(x_train, y_train, x_test, y_test, x_train.shape[1],\n",
    "                     len(set(y_train)),\n",
    "                     numpy_rng=numpy.random.RandomState(123),\n",
    "                     svms=False, nb=True, deepnn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
